{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1d96d9-40ad-43aa-9948-35ca901b9def",
   "metadata": {},
   "source": [
    "# Data Ingestion Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b984b-143c-481c-bdbc-8e5996ab24e0",
   "metadata": {},
   "source": [
    "Ok, so in the last lesson we reviewed data ingestion with llamaindex.  In this lab, we'll move through downloading and parsing the 10k reports for uber and lyft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa953e-15d7-494a-853d-056a6bb8f580",
   "metadata": {},
   "source": [
    "### Downloading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226633a1-502b-42f2-9cde-f1f24da9c184",
   "metadata": {},
   "source": [
    "1. Data Retrieval \n",
    "\n",
    "Begin by making a directory called `data/10k`.\n",
    "\n",
    "```bash\n",
    "mkdir -p 'data/10k/'\n",
    "```\n",
    "\n",
    "And from there, download the following pdf documents.  \n",
    "> Use Wget if possible.  Otherwise simply download the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cf3604-64b0-41d3-a737-cfc60237ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_pdf = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf'\n",
    "lyft_pdf = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac05df79-fe93-4315-83bc-354a74484495",
   "metadata": {},
   "source": [
    "For example, we can download the `uber_2021.pdf` file with the following:\n",
    "\n",
    "* `wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'`\n",
    "\n",
    "So download the `uber_2021` report, and then use similar code to download the `lyft_2021` report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a9428-7d6f-4e43-ba47-ead2853b9a43",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e82770-da80-4b14-92c3-7bbeb0b2f6d7",
   "metadata": {},
   "source": [
    "* Uber report\n",
    "\n",
    "For this section, we'll ask you to write various functions in the `index.py` file.  You can call the try out the functions with the `console.py` file.\n",
    "\n",
    "1. Write a function called `read_doc_text` that given a file_path return the entire contents of text using the `pymupdf` library.\n",
    "\n",
    "2. Then write a function called `build_nodes_from_text` that given the text from our pdf, will return a list of node objects.  Do this by using the `Document` constructor, the `SentenceSplitter`, and then calling the `get_nodes_from_documents` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c2254-efc6-44b1-8a06-da57c9ed1053",
   "metadata": {},
   "source": [
    "3. Now make sure you run the `console.py` file, and take a look at the returned nodes.\n",
    "    * For example, if you look at the `first_node`, what attributes are on there available.\n",
    "    * `first_node.__dict__.keys()`\n",
    "    * What is the `start_char_index`, and the `end_char_index`.\n",
    "    * Is there an embedding at this point? (See that this returns `None`/nothing.)\n",
    "    * What is returned from `node.get_content()`\n",
    "    \n",
    "4. Ok, so now we'll want to manually build our embeddings.  You can see in the `build_embeddings(nodes)` we begin by declaring our `OPEN_AI_KEY` environmental variable, and constructing our embedding model.\n",
    "\n",
    "```python\n",
    "os.environ['OPENAI_API_KEY'] =api_key\n",
    "embed_model = OpenAIEmbedding(api_key=api_key)\n",
    "```\n",
    "\n",
    "Complete the function to manually embed each node, and store the embedding on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279abea-da30-4a06-b56a-413a9de75a2a",
   "metadata": {},
   "source": [
    "5. Now take another look at the first node and confirm that the embedding is stored on the node.\n",
    "* `first_node.embedding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f134fad-991d-40e2-9e2e-4db1cc4e60f7",
   "metadata": {},
   "source": [
    "\n",
    "6. Next, load the nodes to the vector store index.  This should simply use pass through the nodes to the `VectorStoreIndex`.\n",
    "\n",
    "> In truth, this `VectorStoreIndex` function does a few things. (1) It creates the embeddings and stores it on each node (we did it manually for fun), (2) it builds the VectorStore, where these nodes are stored in a simple in memory database, (3) it build the *Index* of the nodes which specifies *how* these nodes are stored.  We'll talk more about indexes later.\n",
    "\n",
    "7. build_query_engine_from(index)\n",
    "\n",
    "Do this by writing a function called `build_query_engine` that calls the `index.as_query_engine` function, with the `tree_summarize` response mode, and then returns the query engine.\n",
    "\n",
    "8. In the console, use the `query_engine.query` function to ask the following.  And store the result as `response`.\n",
    "\n",
    "> \"What is the revenue growth of Uber from 2020 to 2021?\"\n",
    "\n",
    "* You should be able to return text like the following: \n",
    "\n",
    "> 'The revenue growth of Uber from 2020 to 2021 was 57%.'\n",
    "\n",
    "* Also, use `response.source_nodes[0]` to find the original text where this came from.\n",
    "\n",
    "9. Persist and load data\n",
    "\n",
    "* Then call the `persist_data` function, passing through the index to persist the data to disk.  Notice that this creates a new folder called `storage`.Take a look at some of the files in this folder. \n",
    "\n",
    "Finally, in the console.py file, we already imported the `load_index_from_storage` function from the llamaindex library for you.  Use this function to load the index that you saved to your computer, and then create a query engine from the index, and submit your query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d763b4c-a0b4-4ac4-ae10-f41356556b1c",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e59af-4c0d-48c2-aee5-b46d4ee12136",
   "metadata": {},
   "source": [
    "From there, you can move through the following [Llamaindex tutorial](https://docs.llamaindex.ai/en/stable/examples/usecases/10k_sub_question.html), and then re-read this resource on [chunking](https://www.pinecone.io/learn/chunking-strategies/).\n",
    "\n",
    "* Then move through the following on [DataConnectors](https://www.gettingstarted.ai/llamaindex-data-connectors-create-custom-chatgpt-using-own-documents/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
