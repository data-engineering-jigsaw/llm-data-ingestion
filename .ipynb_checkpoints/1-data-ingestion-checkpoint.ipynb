{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aede2b-4060-4fb4-8e5d-606a9845446d",
   "metadata": {},
   "source": [
    "# Data Ingestion - Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ce1de-b62d-459b-94ac-581fb1d5b280",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714c36a-fd27-4157-813d-d57fa12ba6a5",
   "metadata": {},
   "source": [
    "So far we stored entire documents in our database, which then were fed as a context to llm model.  For example, when working with our restaurant review data, an individual review was separately stored and fed to the llm.\n",
    "\n",
    "However, oftentimes we will have longer documents.  In that case, we may have to break up our documents into multiple segments both for storage and to feed to our llm.\n",
    "\n",
    "We'll describe some of the reasoning and techniques for accomplishing this in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e20fdf-609a-4fbe-8d53-b78601fb198b",
   "metadata": {},
   "source": [
    "### Chunks and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238e365-73ad-4040-88f0-9846b0333efd",
   "metadata": {},
   "source": [
    "When our we feed text to an AI model, before being embedded that text is first broken into tokens.  A token is the smallest unit into which text data can be broken down for an AI model to process.  \n",
    "\n",
    "Each word is typically broken into a separate token.  However, as you'll see below, the tokens may also include punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c228f-2076-4306-b3fb-af4991de68ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Download necessary datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I am sleepwalking, are you?\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "# ['I', 'am', 'sleepwalking', ',', 'are', 'you', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428ba07-cd6e-4222-ab36-3fe5f7c52749",
   "metadata": {},
   "source": [
    "With LLMs we do not need to worry too much about the tokenization process -- the llm will tokenize our text for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f11f3-b64b-4348-b303-8d71303901bb",
   "metadata": {},
   "source": [
    "* Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d85d1-1d06-4125-bb84-6dc425c0bb36",
   "metadata": {},
   "source": [
    "What is more relevant to us are the *chunks* that a document is broken, which are then stored in our vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c0921-518b-42f6-9a67-282beaffb322",
   "metadata": {},
   "source": [
    "Remember that we would not want to store an entire pdf file or document in a vector database.  If we did this, then we would be retrieving information that is not so relevant to the user's query.  So instead we chunk our document into different components.\n",
    "\n",
    "Chunking, just means to breaking down large pieces of text into smaller segments.  These chunks will ultimately be embedded into vectors to be stored in our database, and then the relevant chunks will be retrieved and fed to our LLM.  Typically, a chunk size may have a limit of 250 tokens (on the small end) to a few thousand tokens.  And remember a token is about 3/4 of a word.\n",
    "\n",
    "In working with LLMs, you will have some control over the size of the chunk.  And you may wish to change the size of the chunk depending on:\n",
    "\n",
    "* How the content is naturally grouped (among other factors).  For example, for a pdf document, we can imagine chunking a paragraph at a time, and then embedding it.  If we have a dataset of tweets, we may chunk each individual tweet (see more in the chunking strategies resource below.)\n",
    "\n",
    "* The amount of context we can provide to our llm.  For example, GPT4 can be fed more information in it's context than GPT3.5, so the chunks could be larger with chatgpt4.\n",
    "\n",
    "* The kinds of questions a user asks.  Longer questions typically require more information, so if the user is providing longer questions we would want larger chunks.\n",
    "\n",
    "> See more considerations [here](https://www.pinecone.io/learn/chunking-strategies/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e31f2c-4864-4af6-be21-35884fa05eb7",
   "metadata": {},
   "source": [
    "For now we can just set each sentence to be a separate chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da19679-9dcf-432e-ba0a-5128d388ff8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "climate_change_text = \"In common usage, climate change describes global warming—the ongoing increase in global average temperature—and its effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global average temperature is more rapid than previous changes, and is primarily caused by humans burning fossil fuels.[3][4] Fossil fuel use, deforestation, and some agricultural and industrial practices add to greenhouse gases, notably carbon dioxide and methane.[5] Greenhouse gases absorb some of the heat that the Earth radiates after it warms from sunlight. Larger amounts of these gases trap more heat in Earth's lower atmosphere, causing global warming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506361b0-4de0-48ec-b7c2-29a61a2c048f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In common usage, climate change describes global warming—the ongoing increase in global average temperature—and its effects on Earth's climate system\",\n",
       " \" Climate change in a broader sense also includes previous long-term changes to Earth's climate\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = climate_change_text.split('.')\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff8ab6-cde6-45d8-869f-7ab6962ee4da",
   "metadata": {},
   "source": [
    "And remember that at this stage, we can use openai to retrieve the embedding for each chunk, and store text of each chunk along with the embedding in the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49302b-9880-431b-9ecc-c2d250a74226",
   "metadata": {},
   "source": [
    "### Moving to LLamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa967c9-a08f-4bbb-9699-628f380b1f1a",
   "metadata": {},
   "source": [
    "So now that we learned about chunking a document in general, let's move to llamaindex.  Llamaindex is a library for building LLM pipelines.  \n",
    "\n",
    "> An alternative to Llamaindex is langchain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe42beb-4e8b-4afb-abbe-dcab5c15252f",
   "metadata": {},
   "source": [
    "One of the components that LLamaindex helps with is data ingestion.  And in the data ingestion stage, we both retrieve the text information, and chunk the document.\n",
    "\n",
    "Ok, so let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d61de-c43b-4a85-8dc6-c839f9c46476",
   "metadata": {},
   "source": [
    "> The code below is also located in the separate `codebase` folder which may work better than moving through this collab.\n",
    "\n",
    "\n",
    "> Install the requirements.txt, and then run open the index.py file.  Add the openai api key, and run `python3 -i index.py` to interact with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39412a55-3f5f-4f62-ab5e-00cad54a7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pymupdf\n",
    "!pip3 install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0001c26-14f2-4699-b97a-5e79e92e0855",
   "metadata": {},
   "source": [
    "[Located here](https://www.worldbank.org/en/publication/wdr2024) is the world development report on middle income countries from the world bank.  If you click on the link you'll see what we would like to download.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aaaef5-b566-467e-b03f-fe84b46a9bba",
   "metadata": {},
   "source": [
    "> So let's make a new directory and then download the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbfc64cb-a018-43cb-bfac-ef8e91d657a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3b6fb-a3b2-4638-aed9-b5532c072f2f",
   "metadata": {},
   "source": [
    "> You may need to download wget (`brew install wget` on a mac.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c9dc581-42fb-4e27-a563-bd27c1385608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget --user-agent \"Mozilla\" \"https://thedocs.worldbank.org/en/doc/ba24094c4f9f37714f07345b1505c930-0050062023/original/WDR2024-Concept-Note.pdf\" -O \"data/dev_report.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4dc9a-1293-41f1-8e7f-bbccc29b84b2",
   "metadata": {},
   "source": [
    "And from there, we can use pymupdf's fitz module to read the contents of our pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71fe8341-bb8a-4c81-bd66-f87046a222d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/dev_report.pdf\"\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "doc_text = \"\"\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    page_text = page.get_text(\"text\")\n",
    "    doc_text += page_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c59d9-764d-4a7f-9769-641af82221ec",
   "metadata": {},
   "source": [
    "Now that we've collected the text, we can create a Document object with this text (which we do below).  And then use the SentenceSplitter to break this into a chunk size of 1024 tokens, where sentences are not cut off midstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1e24f-d827-4ebe-b80c-c85d8fb2a81c",
   "metadata": {},
   "source": [
    "The `get_nodes_from_documents` function is what chunks our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3428441-0300-40d5-83c4-0f81ca7c8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = [Document(text = doc_text)]\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=1024)\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0816c5-d4c1-4102-aba9-be170c4b7b68",
   "metadata": {},
   "source": [
    "Let's take a look at one of those nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328740ec-5f6c-4162-87bf-3d6f6525c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = nodes[0]\n",
    "node.__dict__.keys()\n",
    "# ['id_', 'embedding', 'metadata', \n",
    "# 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys',\n",
    "# 'relationships', 'text', 'start_char_idx', 'end_char_idx',\n",
    "# 'text_template', 'metadata_template', 'metadata_seperator']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc240fd-a330-4f16-bf1c-d9eb10d71c50",
   "metadata": {},
   "source": [
    "You can see that in addition to storing the text, on each node is also the corresponding embedding.  And then there is also an atribute of relationships -- this includes pointers to the previous node and the next node.  \n",
    "\n",
    "> And something like this is valuable if say, we wish to include say a summary of the previous node's text along with the current node's text to improve our search.\n",
    "\n",
    "Metadata may include information like the page number or name of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493beae-ce8f-4e4c-8197-f613278d38bc",
   "metadata": {},
   "source": [
    "* Adding the embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6322f9-1a0c-4b54-b6c0-0955bfc0b3be",
   "metadata": {},
   "source": [
    "In the next step, we explicitly add an embedding to each node with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73080e58-b5b7-4f84-96b7-765b0157bff7",
   "metadata": {},
   "source": [
    "```python\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content()\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ac592-89f4-444f-be9d-f07f327eb569",
   "metadata": {},
   "source": [
    "You can see that we add the embedding directly to each node by calling:\n",
    "    \n",
    "```\n",
    "node.get_content()\n",
    "```\n",
    "\n",
    "To return the text, and then we pass that text into the `embed_model.get_text_embedding()` function to get the corresponding embedding.  Then we store this embedding on the node's embedding attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee34fc-28ec-4ff2-be9c-40d72bda686b",
   "metadata": {},
   "source": [
    "### Finishing up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f69d51-e91c-4b8a-806a-270ecb0e5f62",
   "metadata": {},
   "source": [
    "Ok, so now we moved through downloading a document with `wget` then reading the text of that document with pymupdf's fitz module, and chunking that text into separate nodes.\n",
    "\n",
    "Let's store those nodes in a vector store database, and then ask a question of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3110c-3020-4f23-a774-b1abf66f299d",
   "metadata": {},
   "source": [
    "So below we store the nodes in a VectorStoreIndex (an in memory vector database), and then specify how we would like a response from the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a4b16-40e2-430a-8251-8853e357d1c3",
   "metadata": {},
   "source": [
    "```python\n",
    "os.environ['OPENAI_API_KEY'] =api_key\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457a96d-73cc-44d8-8cdb-075774231393",
   "metadata": {},
   "source": [
    "> We need to specify our `openai_apikey` in the environment, because the VectorStoreIndex will re-embed our nodes using the openai api.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5286cbd9-9f13-464b-8516-77ceee25dfc5",
   "metadata": {},
   "source": [
    "Finally we can ask questions of our vector store with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612afb6c-1e5d-4e6c-8e66-6120c1963ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is creative destruction according to the document?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e78cca-86a4-4207-b7eb-acb402c45426",
   "metadata": {},
   "source": [
    "And in the response we can both get the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a535272-abe3-4486-9872-e2b3628a4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26176717-8cd0-42fd-8717-613a6169c7cd",
   "metadata": {},
   "source": [
    "As well as the nodes that the query engine used to generate that response.  So if you look at the first source_node, you'll see the relevant text that was retrieved from our original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cd9a8-0bc7-4615-bb3b-c6730bb23512",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9d1dc-5ef7-4e6b-ac6f-8bff6944e142",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23ccdd-5015-4b40-a3d8-25b3e42c1688",
   "metadata": {},
   "source": [
    "Ok, that was a lot but all of our code is can be encapsulated in a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6997b84-7719-47f3-9dff-ebc256a12be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "file_path = \"./data/dev_report.pdf\"\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "doc_text = \"\"\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    page_text = page.get_text(\"text\")\n",
    "    doc_text += page_text\n",
    "\n",
    "\n",
    "documents = [Document(text = doc_text)]\n",
    "parser = SentenceSplitter(chunk_size=1024)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "# api_key = \"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\"\n",
    "# embed_model = OpenAIEmbedding(api_key=api_key)\n",
    "\n",
    "# for node in nodes:\n",
    "#     node_embedding = embed_model.get_text_embedding(\n",
    "#         node.get_content()\n",
    "#     )\n",
    "#     node.embedding = node_embedding\n",
    "\n",
    "# Note: Above, we  skip the embedding as openai automatically embeds when \n",
    "# we pass our nodes into our vector index\n",
    "api_key = \"\"\n",
    "os.environ['OPENAI_API_KEY'] =api_key\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "response = query_engine.query(\"What is creative destruction according to the document?\")\n",
    "\n",
    "response_text = response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800c6f8-675d-4642-bc1f-38de8bec3169",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01792db-0450-4b04-a90b-5a85b1cb6921",
   "metadata": {},
   "source": [
    "[Pinecone Ingestion](https://docs.llamaindex.ai/en/stable/examples/low_level/ingestion.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb64c71-6ce7-411a-ae3c-fb8d36f7643a",
   "metadata": {},
   "source": [
    "[Documents and Nodes](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34247ac9-ab61-40a9-9b85-37c1ba209104",
   "metadata": {},
   "source": [
    "[Karpathy - Tokenization Video](https://www.youtube.com/watch?v=zduSFxRajkE&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62dee6a-d290-4fa2-82ea-4e503a20c59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
